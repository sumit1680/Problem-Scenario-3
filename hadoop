//Import all tables from mysql database into hdfs as avro data files. 
use compression and the compression codec should be snappy. data warehouse directory should be retail_stage.db

sqoop import-all-tables \
-Dmapreduce.job.user.classpath.first=true \
--connect jdbc:mysql://ms.itversity.com/retail_db" \
--username retail_user \
--password itversity \
--compress \
--compression-codec Snappy \
--warehouse-dir /user/sumitsinha1680/hive/warehouse/retail_stage4.db \
--as-avrodatafile \
-m 1;

//in case hadoop not able to read avrodata file.Please use below code
-Dmapreduce.job.user.classpath.first=true \

//Create a metastore table that should point to the orders data imported by sqoop job above. Name the table orders_sqoop
mkdir avro-test
cd avro-test
hadoop fs -get /user/hive/warehouse/retail_stage4.db/orders/part-m-00000.avro
avro-tools getschema part-m-00000.avro > orders.avsc

//Now we need to load the avsc file to hdfc
hadoop fs -mkdir /user/sumitsinha1680/hive/schemas
hadoop fs -mkdir /user/sumitsinha1680/hive/schemas/order
hadoop fs -copyFromLocal orders.avsc /user/hive/schemas/order

Launch HIVE using 'hive' command in a separate terminal

Below HIVE command will create a table pointing to the avro data file for orders data

create external table orders2_sqoop
STORED AS AVRO
LOCATION '/user/sumitsinha1680/hive/warehouse/retail_stage4.db/orders'
TBLPROPERTIES ('avro.schema.url'='/user/sumitsinha1680/hive/schemas/order/orders.avsc')

q3 - //Write query in hive that shows all orders belonging to a certain day. 
This day is when the most orders were placed. select data from orders2_sqoop.

//first find the total orders for a given date in descending order
Run ih hive

select y.order_date,count(1) as total_orders from orders2_sqoop as y group by
y.order_date order by total_orders desc limit 1;

//we get order date and highest orders and i need date only
select z.order_date from (select y.order_date,count(1) as total_orders from orders2_sqoop as y group by
y.order_date order by total_orders desc limit 1) as z;

//extract all the columns
select * from orders2_sqoop as x where x.order_date in (select z.order_date from (select y.order_date,count(1) as total_orders from orders2_sqoop as y 
group by y.order_date order by total_orders desc limit 1) as z);

//04-query table in impala that shows all orders belonging to a certain day. 
This day is when the most orders were placed. select data from order_sqoop. 

Lanch Impala shell by using command impala-shell

1. Run 'Invalidate metadata'
2. Run below query


select * from orders_sqoop as X where X.order_date in (select a.order_date from (select Y.order_date, count(1) as total_orders from orders_sqoop as Y group by Y.order_date order by total_orders desc, Y.order_date desc limit 1) a);

//Now create a table named retail.orders_avro in hive stored as avro, the table should have same table definition as order_sqoop. 
Additionally, this new table should be partitioned by the order month i.e -> year-order_month.(example: 2014-01)

//enable dynamic partition
set hive.exec.dynamic.partition = true;
set hive.exec.dynamic.partition.mode = nonstrict;

insert overwrite table orders_avro partition (order_month)
select order_id, to_date(from_unixtime(cast(order_date/1000 as int))), 
order_customer_id, order_status, 
substr(from_unixtime(cast(order_date/1000 as int)),1,7) as order_month from default.orders2_sqoop;

//Write query in hive that shows all orders belonging to a certain day. 
This day is when the most orders were placed. select data from orders_avro

select * from orders_avro as X where X.order_date in (select a.order_date from (select Y.order_date, count(1) as total_orders from orders_avro 
as Y group by Y.order_date order by total_orders desc, Y.order_date desc limit 1) a);

//evolve the avro schema related to orders_sqoop table by adding more fields named (order_style String, order_zone Integer)
insert two more records into orders_sqoop table

1. hadoop fs -get /user/hive/schemas/order/orders.avsc
2. gedit orders.avsc

3.{
  "type" : "record",
  "name" : "orders",
  "doc" : "Sqoop import of orders",
  "fields" : [ {
    "name" : "order_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_id",
    "sqlType" : "4"
  }, {
    "name" : "order_date",
    "type" : [ "null", "long" ],
    "default" : null,
    "columnName" : "order_date",
    "sqlType" : "93"
  }, {
    "name" : "order_customer_id",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_customer_id",
    "sqlType" : "4"
  },{
    "name" : "order_style",
    "type" : [ "null", "string" ],
    "default" : null,
    "columnName" : "order_style",
    "sqlType" : "12"
  }, {
    "name" : "order_zone",
    "type" : [ "null", "int" ],
    "default" : null,
    "columnName" : "order_zone",
    "sqlType" : "4"
  }, {
    "name" : "order_status",
    "type" : [ "null", "string" ],
    "default" : null,
    "columnName" : "order_status",
    "sqlType" : "12"
  } ],
  "tableName" : "orders"
}

4. hadoop fs -copyFromLocal -f orders.avsc /user/hive/schemas/order/orders.avsc

Step 9 - Insert 2 records from Hive shell
insert into table orders_sqoop values (8888888,1374735600000,11567,"xyz",9,"CLOSED");
insert into table orders_sqoop values (8888889,1374735600000,11567,"xyz",9,"CLOSED");
Step 10 -Run the query in Hive: 
Run this query in Hive. 

select * from orders_sqoop as X where X.order_date in (select inner.order_date from (select Y.order_date, count(1) as total_orders from orders_sqoop as Y group by Y.order_date order by total_orders desc, Y.order_date desc limit 1) inner);



Step 11-Run the query Impala: 
Lanch Impala shell by using command impala-shell

1. Run 'Invalidate metadata'
2. Run below query


select * from orders_sqoop as X where X.order_date in (select a.order_date from (select Y.order_date, count(1) as total_orders from orders_sqoop as Y group by Y.order_date order by total_orders desc, Y.order_date desc limit 1) a);











